{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Family\n",
    "Gradient Descent is an optimization algorithm to get the minimum error for a given function, It has several variants, some are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the function, we have to find minima for\n",
    "def fn(x):\n",
    "    return (x - 5)**4 - 2 * (x**2) + 21 * x\n",
    "\n",
    "# This is its derivate, will give us the slope at a given point\n",
    "def fn_deriv(x):\n",
    "    return 4 * (x - 5)**3 - 4 * (x - 5) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see in this image, there is a local maxima at 5.27, local minima at 5.838 and global minima at 3.893\n",
    "\n",
    "![oops](http://i.imgur.com/2JuheEF.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create classes for some Gradient Descent implementations/variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StochasticGradientDescent:\n",
    "    def __init__(self, func, func_deriv, learning_rate = 0.01):\n",
    "        self.fn = func\n",
    "        self.fn_deriv = func_deriv\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def minimize(self, time_steps, x):\n",
    "        for n in range(time_steps):\n",
    "            x = x - self.learning_rate * fn_deriv(x)\n",
    "        return x.real\n",
    "    \n",
    "    def maximize(self, time_steps, x):\n",
    "        for n in range(time_steps):\n",
    "            x = x + self.learning_rate * fn_deriv(x)\n",
    "        return x.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.892840128311234\n",
      "5.837565435283333\n",
      "5.269594436405425\n"
     ]
    }
   ],
   "source": [
    "SGD = StochasticGradientDescent(fn, fn_deriv)\n",
    "print(SGD.minimize(1000, 5.))\n",
    "print(SGD.minimize(1000, 6.5))\n",
    "print(SGD.maximize(1000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MomentumGradientDescent:\n",
    "    def __init__(self, func, func_deriv, alpha = 0.01, learning_rate = 0.01):\n",
    "        self.fn = func\n",
    "        self.fn_deriv = func_deriv\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def minimize(self, time_steps, x):\n",
    "        delta = 0\n",
    "        for n in range(time_steps):\n",
    "            delta = - self.learning_rate * fn_deriv(x) + delta * self.alpha\n",
    "            x = x + delta\n",
    "        return x.real\n",
    "    \n",
    "    def maximize(self, time_steps, x):\n",
    "        delta = 0\n",
    "        for n in range(time_steps):\n",
    "            delta = + self.learning_rate * fn_deriv(x) + delta * self.alpha\n",
    "            x = x + delta\n",
    "        return x.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.892840128311234\n",
      "5.837565435283333\n",
      "5.269594436405431\n"
     ]
    }
   ],
   "source": [
    "MGD = MomentumGradientDescent(fn, fn_deriv)\n",
    "print(MGD.minimize(1000, 5.))\n",
    "print(MGD.minimize(1000, 6.5))\n",
    "print(MGD.maximize(1000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.892840128311234\n"
     ]
    }
   ],
   "source": [
    "# But what if I drop from a little higher? :P\n",
    "print(MGD.minimize(1000, 10.5))\n",
    "\n",
    "# It will jump that little bump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdamGD:\n",
    "    def __init__(self, func, func_deriv, beta1, beta2, learning_rate, epsilon):\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = 0.\n",
    "        self.velocity = 0.\n",
    "        self.fn = func\n",
    "        self.fn_deriv = func_deriv\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def minimize(self, time_steps, x):\n",
    "        for n in range(time_steps):\n",
    "            self.momentum = self.beta1 * self.momentum + (1 - self.beta1) * self.fn_deriv(x)\n",
    "            self.velocity = self.beta2 * self.velocity + (1 - self.beta2) * self.fn_deriv(x)**2\n",
    "            moment = self.momentum / (1 - self.beta1)\n",
    "            vel_moment = self.momentum / (1 - self.beta2)\n",
    "            \n",
    "            x = x - self.learning_rate * (moment / (vel_moment ** (.5) + self.epsilon ))\n",
    "        return x.real\n",
    "    \n",
    "    def maximize(self, time_steps, x):\n",
    "        for n in range(time_steps):\n",
    "            self.momentum = self.beta1 * self.momentum + (1 - self.beta1) * self.fn_deriv(x)\n",
    "            self.velocity = self.beta2 * self.velocity + (1 - self.beta2) * self.fn_deriv(x)**2\n",
    "            moment = self.momentum / (1 - self.beta1)\n",
    "            vel_moment = self.momentum / (1 - self.beta2)\n",
    "            \n",
    "            x = x + self.learning_rate * (moment / (vel_moment ** (.5) + self.epsilon ))\n",
    "        return x.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.551452976387318\n",
      "5.755233999413476\n",
      "5.331964771693469\n"
     ]
    }
   ],
   "source": [
    "Adam = AdamGD(fn, fn_deriv, beta1 = 0.001, beta2=0.001, learning_rate = 0.01, epsilon=0.0001)\n",
    "print(Adam.minimize(1000, 5))\n",
    "print(Adam.minimize(1000, 6.5))\n",
    "print(Adam.maximize(1000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.758129799520977\n"
     ]
    }
   ],
   "source": [
    "# But what if.... :P\n",
    "print(Adam.minimize(1000, 10.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So Adam fixed the issue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
